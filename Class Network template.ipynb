{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer():\n",
    "    \n",
    "    def __init__(self, n_nodes, act='logistic'):\n",
    "        '''\n",
    "            lyr = Layer(n_nodes, act='logistic')\n",
    "            \n",
    "            Creates a layer object.\n",
    "            \n",
    "            Inputs:\n",
    "             n_nodes  the number of nodes in the layer\n",
    "             act      specifies the activation function\n",
    "                      Use 'logistic' or 'identity'\n",
    "        '''\n",
    "        self.N = n_nodes  # number of nodes in this layer\n",
    "        self.h = []      # node activities\n",
    "        self.z = []\n",
    "        self.b = np.zeros(self.N)  # biases\n",
    "        \n",
    "        # Activation functions\n",
    "        self.sigma = self.Logistic\n",
    "        self.sigma_p = (lambda : self.Logistic_p())\n",
    "        if act=='identity':\n",
    "            self.sigma = self.Identity\n",
    "            self.sigma_p = (lambda : self.Identity_p())\n",
    "       \n",
    "    def Logistic(self):\n",
    "        return 1. / (1. + np.exp(-self.z))\n",
    "    def Logistic_p(self):\n",
    "        return self.h * (1.-self.h)\n",
    "    def Identity(self):\n",
    "        return self.z\n",
    "    def Identity_p(self):\n",
    "        return np.ones_like(self.h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network():\n",
    "\n",
    "    \n",
    "    def FeedForward(self, x):\n",
    "        '''\n",
    "            y = net.FeedForward(x)\n",
    "\n",
    "            Runs the network forward, starting with x as input.\n",
    "            Returns the activity of the output layer.\n",
    "\n",
    "            All node use Logistic\n",
    "            Note: The activation function used for the output layer\n",
    "            depends on what self.Loss is set to.\n",
    "        '''\n",
    "        # initialize variables.\n",
    "        x = np.array(x)  # Convert input to array, in case it's not\n",
    "        N = NSamples(x)\n",
    "        self.lyr[0].h = x\n",
    "        # Loop through all layers.\n",
    "        for i in range(self.n_layers-1):\n",
    "            # Construct Weight and Bias matrix\n",
    "            weights = self.W[i]\n",
    "            # Update next layer's income currents and activities.\n",
    "            self.lyr[i+1].z = np.matmul(self.lyr[i].h, weights) + self.lyr[i+1].b\n",
    "            self.lyr[i+1].h = self.lyr[i+1].sigma()\n",
    "        return self.lyr[-1].h\n",
    "\n",
    "    \n",
    "    def BackProp(self, t, lrate=0.05):\n",
    "        '''\n",
    "            net.BackProp(targets, lrate=0.05)\n",
    "            \n",
    "            Given the current network state and targets t, updates the connection\n",
    "            weights and biases using the backpropagation algorithm.\n",
    "            \n",
    "            Inputs:\n",
    "             t      an array of targets (number of samples must match the\n",
    "                    network's output)\n",
    "             lrate  learning rate\n",
    "        '''\n",
    "        t = np.array(t)  # convert t to an array, in case it's not\n",
    "        # Initialize variables.\n",
    "        N = NSamples(t)\n",
    "        dEdh = self.gradLoss(self.lyr[-1].h, t)\n",
    "        dhdz = self.lyr[-1].sigma_p()\n",
    "        dEdz = dhdz * dEdh\n",
    "        for ind in range(len(self.lyr)-2 ,-1, -1):\n",
    "            weights = self.W[ind]\n",
    "            dense_dEdb = np.array([sum(x) for x in zip(*dEdz)])\n",
    "\n",
    "            matrix_dEdW = (dEdz.T @ self.lyr[ind].h).T\n",
    "            dhdz = self.lyr[ind].sigma_p()\n",
    "            dEdz = np.multiply(dhdz, np.matmul(dEdz, weights.T))\n",
    "            self.lyr[ind+1].b -= lrate * dense_dEdb \n",
    "            self.W[ind] -= lrate * matrix_dEdW \n",
    "            \n",
    "    def Learn(self, inputs, targets, lrate=0.05, epochs=1, progress=True):\n",
    "        '''\n",
    "            Network.Learn(data, lrate=0.05, epochs=1, progress=True)\n",
    "\n",
    "            Run through the dataset 'epochs' number of times, incrementing the\n",
    "            network weights after each epoch. For each epoch, it\n",
    "            shuffles the order of the samples.\n",
    "\n",
    "            Inputs:\n",
    "              data is a list of 2 arrays, one for inputs, and one for targets\n",
    "              lrate is the learning rate (try 0.001 to 0.5)\n",
    "              epochs is the number of times to go through the training data\n",
    "              progress (Boolean) indicates whether to show cost\n",
    "        '''\n",
    "        for _ in range(epochs):\n",
    "            y = self.FeedForward(inputs)\n",
    "            self.BackProp(targets, lrate)\n",
    "            if progress:\n",
    "                self.cost_history.append(self.Loss(self.lyr[-1].h, targets))\n",
    "        if progress:\n",
    "            self.cost_history.append(self.Evaluate(inputs, targets))\n",
    "    \n",
    "    \n",
    "    def __init__(self, sizes, type='classifier'):\n",
    "        '''\n",
    "            net = Network(sizes, type='classifier')\n",
    "\n",
    "            Creates a Network and saves it in the variable 'net'.\n",
    "\n",
    "            Inputs:\n",
    "              sizes is a list of integers specifying the number\n",
    "                  of nodes in each layer\n",
    "                  eg. [5, 20, 3] will create a 3-layer network\n",
    "                      with 5 input, 20 hidden, and 3 output nodes\n",
    "              type can be either 'classifier' or 'regression', and\n",
    "                  sets the activation function on the output layer,\n",
    "                  as well as the loss function.\n",
    "                  'classifier': logistic, cross entropy\n",
    "                  'regression': linear, mean squared error\n",
    "        '''\n",
    "        self.n_layers = len(sizes)\n",
    "        self.lyr = []    # a list of Layers\n",
    "        self.W = []      # Weight matrices, indexed by the layer below it\n",
    "        \n",
    "        self.cost_history = []  # keeps track of the cost as learning progresses\n",
    "        \n",
    "        # Two common types of networks\n",
    "        # The member variable self.Loss refers to one of the implemented\n",
    "        # loss functions: MSE, or CrossEntropy.\n",
    "        # Call it using self.Loss(t)\n",
    "        if type=='classifier':\n",
    "            self.classifier = True\n",
    "            self.Loss = CrossEntropy\n",
    "            self.gradLoss = gradCrossEntropy\n",
    "            activation = 'logistic'\n",
    "        else:\n",
    "            self.classifier = False\n",
    "            self.Loss = MSE\n",
    "            self.gradLoss = gradMSE\n",
    "            activation = 'identity'\n",
    "\n",
    "        # Create and add Layers (using logistic for hidden layers)\n",
    "        for n in sizes[:-1]:\n",
    "            self.lyr.append( Layer(n) )\n",
    "   \n",
    "        # For the top layer, we use the appropriate activtaion function\n",
    "        self.lyr.append( Layer(sizes[-1], act=activation) )\n",
    "    \n",
    "        # Randomly initialize weight matrices\n",
    "        for idx in range(self.n_layers-1):\n",
    "            m = self.lyr[idx].N\n",
    "            n = self.lyr[idx+1].N\n",
    "            temp = np.random.normal(size=[m,n])/np.sqrt(m)\n",
    "            self.W.append(temp)\n",
    "\n",
    "    def Evaluate(self, inputs, targets):\n",
    "        '''\n",
    "            E = net.Evaluate(data)\n",
    "\n",
    "            Computes the average loss over the supplied dataset.\n",
    "\n",
    "            Inputs\n",
    "             inputs  is an array of inputs\n",
    "             targets is a list of corresponding targets\n",
    "\n",
    "            Outputs\n",
    "             E is a scalar, the average loss\n",
    "        '''\n",
    "        y = self.FeedForward(inputs)\n",
    "        return self.Loss(y, targets)\n",
    "\n",
    "    def ClassificationAccuracy(self, inputs, targets):\n",
    "        '''\n",
    "            a = net.ClassificationAccuracy(data)\n",
    "            \n",
    "            Returns the fraction (between 0 and 1) of correct one-hot classifications\n",
    "            in the dataset.\n",
    "        '''\n",
    "        y = self.FeedForward(inputs)\n",
    "        yb = OneHot(y)\n",
    "        n_incorrect = np.sum(yb!=targets) / 2.\n",
    "        return 1. - float(n_incorrect) / NSamples(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
